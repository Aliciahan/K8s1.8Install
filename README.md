<h1 align="center"> Kunernetes 1.8.2 Installation in CentOS 7 </h1> 

---

<h2 id="archi">Architecture</h2>

The landscape we're going to build: 

| Machine | IP Address | Service Running | 
|:----------------|:----------------|:----------|
| M134 | 192.168.94.134 | etcd, flanneld, kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, kube-proxy |
| M136 | 192.168.94.136 | etcd, flanneld,  kubelet, kube-proxy |


<h2 id="etcd"> ETCD Configuration </h2>

Before start, make sure that the firewall of CentOS has been closed:

~~~bash 
systemctl disable firewalld
systemctl stop firewalld
~~~

### The Certificates: 

There exists several ways doing the same thing, personally I respect the way proposed by CoreOS at: <a href="https://coreos.com/etcd/docs/3.2.7/op-guide/clustering.html">Clustering etcd TLS Configuration</a>. 

1. Firstly, download from cfssl( <a href="https://pkg.cfssl.org">Downloadpage</a> ) these two bins: 

~~~bash
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64

mv ./cfssl_linux-amd64 /usr/bin/cfssl && chmod +x /usr/bin/cfssl
mv ./cfssljson_linux-amd64 /usr/bin/cfssljson && chmod +x /usr/bin/cfssljson
~~~

2. clone the git repo of edcd :

> git clone https://github.com/coreos/etcd.git 

3. Copy the  etcd/hack/tls-setup repository to /root/tls-setup

4. Configure the /root/req-csr.json file put your personal ip address after the localhost

~~~json

[root@localhost tls-setup]# cat config/req-csr.json
{
  "CN": "etcd",
  "hosts": [
    "localhost",
    "192.168.94.136",
    "192.168.94.134",
    "192.168.94.131",
    "192.168.94.132",
    "192.168.94.133",
    "192.168.94.135",
    "192.168.94.137",
    "192.168.94.138"
  ],
  "key": {
    "algo": "ecdsa",
    "size": 384
  },
  "names": [
    {
      "O": "autogenerated",
      "OU": "etcd cluster",
      "L": "the internet"
    }
  ]
}
~~~

5. Install "make" and make run **make**


~~~bash
[root@localhost tls-setup]# cat Makefile
.PHONY: cfssl ca req clean

CFSSL	= @env PATH=$(GOPATH)/bin:$(PATH) cfssl
JSON	= env PATH=$(GOPATH)/bin:$(PATH) cfssljson

all: ca req

#cfssl:
#	go get -u -tags nopkcs11 github.com/cloudflare/cfssl/cmd/cfssl
#	go get -u github.com/cloudflare/cfssl/cmd/cfssljson
#	go get -u github.com/mattn/goreman

ca:
	mkdir -p certs
	$(CFSSL) gencert -initca config/ca-csr.json | $(JSON) -bare certs/ca

req:
	$(CFSSL) gencert \
	  -ca certs/ca.pem \
	  -ca-key certs/ca-key.pem \
	  -config config/ca-config.json \
	  config/req-csr.json | $(JSON) -bare certs/etcd1
	$(CFSSL) gencert \
	  -ca certs/ca.pem \
	  -ca-key certs/ca-key.pem \
	  -config config/ca-config.json \
	  config/req-csr.json | $(JSON) -bare certs/etcd2
	$(CFSSL) gencert \
	  -ca certs/ca.pem \
	  -ca-key certs/ca-key.pem \
	  -config config/ca-config.json \
	  config/req-csr.json | $(JSON) -bare certs/etcd3
	$(CFSSL) gencert \
	  -ca certs/ca.pem \
	  -ca-key certs/ca-key.pem \
	  -config config/ca-config.json \
	  config/req-csr.json | $(JSON) -bare certs/proxy1

clean:
	rm -rf certs
~~~

6. Distribute them to machines

For each machine: 

~~~bash
ssh root@$M134_IP yum install -y etcd
ssh root@$M134_IP mkdir -p /etc/etcd/ssl
scp /root/tls-setup/certs/ca.pem root@$M134:/etc/etcd/ssl/
scp /root/tls-setup/certs/ca-key.pem root@$M134:/etc/etcd/ssl/
scp /root/tls-setup/certs/etcd1.* root@$M134:/etc/etcd/ssl/
ssh root@$M134 chown -R etcd:etcd /etc/etcd/ssl
ssh root@$M134 "chmod -R 644 /etc/etcd/ssl/*"
ssh root@$M134 chmod 755 /etc/etcd/ssl
~~~

7. Edit /usr/lib/systemd/system/etcd.service 

In fact we can personalize the variable in /etc/etcd/etcd.conf. But for the purpose of easy maintenance, I choose to edit directly the service:

~~~
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd \
  --name "M136" \
  --initial-advertise-peer-urls https://192.168.94.136:2380 \
  --listen-peer-urls https://192.168.94.136:2380 \
  --listen-client-urls https://192.168.94.136:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://192.168.94.136:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster M134=https://192.168.94.134:2380,M136=https://192.168.94.136:2380 \
  --initial-cluster-state new \
  --client-cert-auth \
  --trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --cert-file=/etc/etcd/ssl/etcd2.pem \
  --key-file=/etc/etcd/ssl/etcd2-key.pem \
  --peer-client-cert-auth \
  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --peer-cert-file=/etc/etcd/ssl/etcd2.pem \
  --peer-key-file=/etc/etcd/ssl/etcd2-key.pem

Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
~~~

8. Restart Service:

> systemctl daemon-reload
> systemctl restart etcd

9. Verify the etcd nodes

**Attention** we have done the tls way, thus ancient way of verify will not work, we should indicate the pem certificate in our command: 

~~~bash
[root@localhost kubernetes1.8Install]# etcdctl --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd1.pem --key=/etc/etcd/ssl/etcd1-key.pem --endpoints=https://192.168.94.134:2379,https://192.168.94.136:2379 endpoint health
https://192.168.94.134:2379 is healthy: successfully committed proposal: took = 28.7663ms
https://192.168.94.136:2379 is healthy: successfully committed proposal: took = 22.780332ms
[root@localhost kubernetes1.8Install]# etcdctl --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd1.pem --key=/etc/etcd/ssl/etcd1-key.pem --endpoints=https://192.168.94.134:2379,https://192.168.94.136:2379 member list
157821c7d00252a2, started, M136, https://192.168.94.136:2380, https://192.168.94.136:2379
6d28ea3bc3b20778, started, M134, https://192.168.94.134:2380, https://192.168.94.134:2379
~~~

<h2 id="flannel"> Installation Flannel </h2>


1. Before Start 

Install Docker and make it running. Verify that the etcd services are running.

> yum install -y docker && systemctl enable docker && systemctl start docker

2. Generate certificates

We are going to use the same ca.pem and ca-config.json of etcd. But generate the new flannel pem files ourselves. I wrote a script ./tls-setup/flanneld/makeCertFlannel.sh 

~~~bash 

#!/bin/bash

cat > flanneld-csr.json <<EOF
{
  "CN": "flanneld",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF

#!/bin/bash

cfssl gencert -ca=../certs/ca.pem \
  -ca-key=../certs/ca-key.pem \
  -config=../config/ca-config.json \
  flanneld-csr.json | cfssljson -bare flanneld
#>
~~~



3. Distribute these files 

> mkdir -p /etc/flanneld/ssl
> cp flanneld\* /etc/flanneld/ssl

4. Install flanneld

> yum install -y flannel

5. Configure flannel service 

Here is an example : 

~~~bash
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/flanneld
EnvironmentFile=-/etc/sysconfig/docker-network
ExecStart=/usr/bin/flanneld-start \
  -etcd-cafile=/etc/etcd/ssl/ca.pem \
  -etcd-certfile=/etc/flanneld/ssl/flanneld.pem \
  -etcd-keyfile=/etc/flanneld/ssl/flanneld-key.pem \
  -etcd-endpoints=https://192.168.94.134:2379,https://192.168.94.136:2379 \
  -etcd-prefix=/kubernetes/network \
  --iface=ens33
ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
~~~

Pay attention that, for machines have multiple network card, you should indicate the card for communication, with the argument: --iface=xxx. 


6. Restart flanneld, and docker. 

After all these reloading, finally, we should find the network as following: 

~~~bash

docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 172.30.75.1  netmask 255.255.255.0  broadcast 0.0.0.0
        ether 02:42:62:ae:32:33  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

ens33: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.94.136  netmask 255.255.255.0  broadcast 192.168.94.255
        inet6 fe80::b2a2:6704:f908:c3f7  prefixlen 64  scopeid 0x20<link>
        ether 00:0c:29:23:3d:7b  txqueuelen 1000  (Ethernet)
        RX packets 718373  bytes 122584723 (116.9 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 682313  bytes 75599413 (72.0 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

flannel.1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
        inet 172.30.75.0  netmask 255.255.255.255  broadcast 0.0.0.0
        inet6 fe80::440:fbff:fe92:6d48  prefixlen 64  scopeid 0x20<link>
        ether 06:40:fb:92:6d:48  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 8 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1  (Boucle locale)
        RX packets 4674014  bytes 454004451 (432.9 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 4674014  bytes 454004451 (432.9 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
~~~

<h2 id="master"> Master Node Configuration</h2>

1. Download Kubernetes Release 

From https://github.com/kubernetes/kubernetes/releases download the recent release.
download the tar.gz file. 

2. Run get script :

Extract the tar file with tar xvf xxxx.tar.gz then :

> cd kubernetes/cluster && ./get-kube-binaries.sh

3. Go to kuberntes/server repository extract the kubernetes-server-linux-amd64.tar.gz

4. Distribute the files:

~~~bash 
[root@localhost server]# cd bin/
[root@localhost bin]# pwd
/root/kubernetes/server/kubernetes/server/bin
[root@localhost bin]# cp ./kube-apiserver /usr/bin/
[root@localhost bin]# cp ./kube-controller-manager /usr/bin
[root@localhost bin]# cp ./kubectl /usr/bin
[root@localhost bin]# cp ./kube-scheduler /usr/bin
[root@localhost bin]# cp ./kubelet /usr/bin
[root@localhost bin]# cp ./kube-proxy /usr/bin
[root@localhost bin]#
[root@localhost bin]#
[root@localhost bin]# scp ./kubelet ./kube-proxy root@192.168.94.136:/usr/bin/
~~~

5. Generate Certificates :

As we always do: 

- json file:

~~~json
[root@localhost kubernetes]# cat kubernetes-csr.json
{
  "CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "192.168.94.134",
    "172.30.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
~~~

- generate certificates then distribute 

~~~bash
[root@localhost kubernetes]# cat getCert.sh
#!/bin/bash

cfssl gencert -ca=/etc/etcd/ssl/ca.pem \
  -ca-key=../certs/ca-key.pem \
  -config=../config/ca-config.json \
  kubernetes-csr.json | cfssljson -bare kubernetes &&\
mkdir -p /etc/kubernetes/ssl &&\
cp kubernetes* /etc/kubernetes/ssl
#*
~~~

- make tokens for bootstrap

~~~bash 
[root@localhost kubernetes]# cat makeToken.sh
head -c 16 /dev/urandom | od -An -t x | tr -d " " > ./randomToken
TOKEN=$(cat ./randomToken)
cat > /etc/kubernetes/token.csv << EOF
${TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF
~~~

Do another time for Kubectl

- write json 

~~~json

[root@localhost kubectl]# cat admin-csr.json
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}

~~~

- apply it with cfssl 

~~~bash 

[root@localhost kubectl]# cat getCert.sh
#!/bin/bash

cfssl gencert -ca=/etc/etcd/ssl/ca.pem \
  -ca-key=/etc/etcd/ssl/ca-key.pem \
  -config=../config/ca-config.json \
  admin-csr.json | cfssljson -bare admin

EXCODE=$?
if [ "$EXCODE" == "0" ]; then
    if [[ -d /etc/kubernetes/ssl ]]; then
        cp ./admin* /etc/kubernetes/ssl/
    else
        mkdir -p /etc/kubernetes/ssl && \
        cp ./admin* /etc/kubernetes/ssl/
    fi
fi

~~~



6. Setup kube-apiserver systemd service

~~~bash
[root@localhost kubectl]# cat /usr/lib/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
User=root
ExecStart=/usr/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
  --advertise-address=192.168.94.134 \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/lib/audit.log \
  --authorization-mode=Node,RBAC \
  --bind-address=192.168.94.134 \
  --client-ca-file=/etc/etcd/ssl/ca.pem \
  --enable-swagger-ui=true \
  --etcd-cafile=/etc/etcd/ssl/ca.pem \
  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \
  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \
  --etcd-servers=https://192.168.94.134:2379,https://192.168.94.136:2379 \
  --event-ttl=1h \
  --kubelet-https=true \
  --insecure-bind-address=192.168.94.134 \
  --runtime-config=rbac.authorization.k8s.io/v1alpha1 \
  --service-account-key-file=/etc/etcd/ssl/ca-key.pem \
  --service-cluster-ip-range=10.254.0.0/16 \
  --service-node-port-range=30000-31000 \
  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --experimental-bootstrap-token-auth \
  --token-auth-file=/etc/kubernetes/token.csv \
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
~~~

> systemctl daemon-reload && systemctl start kube-apiserver

7. Setup Controller Manager Service

~~~bash

[root@localhost kubectl]# cat /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/bin/kube-controller-manager \
  --address=127.0.0.1 \
  --allocate-node-cidrs=true \
  --cluster-cidr=172.30.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/etc/etcd/ssl/ca.pem \
  --cluster-signing-key-file=/etc/etcd/ssl/ca-key.pem \
  --leader-elect=true \
  --master=http://192.168.94.134:8080 \
  --root-ca-file=/etc/etcd/ssl/ca.pem \
  --service-account-private-key-file=/etc/etcd/ssl/ca-key.pem \
  --service-cluster-ip-range=10.254.0.0/16 \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
~~~

8. Setup kube-scheduler

~~~bash

[root@localhost kubectl]# cat /usr/lib/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/bin/kube-scheduler \
  --leader-elect=true \
  --master=http://192.168.93.134:8080 \
  --address=127.0.0.1 \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target

~~~

9. Configure kubectl

~~~bash
[root@localhost kubectl]# cat setupKubectl.sh
#!/bin/bash

kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/etcd/ssl/ca.pem \
  --embed-certs=true \
  --server=https://192.168.94.134:6443
kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/ssl/admin-key.pem
kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin
kubectl config use-context kubernetes

~~~

**DON'T FORGET to distribute the ~/.kube/config to all nodes running kubelet**

10. Test Kubectl :

~~~bash
[root@localhost kubectl]# kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health": "true"}
etcd-1               Healthy   {"health": "true"}

~~~

<h2 id="node"> Node Configuration </h2>

1. add role to cluster (configure kubectl)

> kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap

--user=kubelet-bootstrap is the username indicated in /etc/kubernetes/token.csv

This is written in to the /etc/kubernetes/bootstrap.config

2. Copy kubelet kube-proxy to Node

Already done

> mkdir -p /var/lib/kubelet

3. Build bootstrap.config file

~~~bash
[root@localhost kubernetes1.8Install]# cat ./configBootStrapConfig.sh
#!/bin/bash

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://192.168.94.134:6443 \
  --kubeconfig=bootstrap.kubeconfig
# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
    --token=$(cat ./tls-setup/kubernetes/randomToken) \
  --kubeconfig=bootstrap.kubeconfig
# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig
# 设置默认上下文
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
cp bootstrap.kubeconfig /etc/kubernetes/
~~~

4. Configure Kubelet service :

~~~bash
[root@localhost kubernetes1.8Install]# cat /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/bin/kubelet \
  --address=192.168.94.134 \
  --hostname-override=192.168.94.134 \
  --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \
  --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \
  --kubeconfig=/etc/kubernetes/kubelet-kubeconfig \
  --cert-dir=/etc/kubernetes/ssl \
  --container-runtime=docker \
  --cluster-dns=10.254.0.2 \
  --cluster-domain=cluster.local. \
  --hairpin-mode promiscuous-bridge \
  --allow-privileged=true \
  --serialize-image-pulls=false \
  --register-node=true \
  --logtostderr=true \
  --fail-swap-on=false \
  --cgroup-driver=systemd \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
~~~

5. Start Service with systemctl 

6. Approve the certificate

~~~bash
[root@localhost kubernetes1.8Install]# kubectl get csr
NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-erT8KExDNg_qHlvGOzGCndwK4mZBslTBHGpXuZAIPyM   28s       kubelet-bootstrap   Pending

[root@localhost kubernetes1.8Install]# kubectl certificate approve node-csr-erT8KExDNg_qHlvGOzGCndwK4mZBslTBHGpXuZAIPyM
certificatesigningrequest "node-csr-erT8KExDNg_qHlvGOzGCndwK4mZBslTBHGpXuZAIPyM" approved

[root@localhost kubernetes1.8Install]# kubectl get nodes
NAME             STATUS    ROLES     AGE       VERSION
192.168.94.134   Ready     <none>    6s        v1.8.2
~~~

<h2 id="kube-proxy"> Configure Kube-Proxy</h2>


1. Generate Certificates :

~~~json
[root@localhost kube-proxy]# cat kube-proxy-csr.json
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
~~~

- get pem files

~~~bash
[root@localhost kube-proxy]# cat getCsr.sh
#!/bin/bash

cfssl gencert -ca=../certs/ca.pem \
  -ca-key=../certs/ca-key.pem \
  -config=../config/ca-config.json \
  kube-proxy-csr.json | cfssljson -bare kube-proxy &&\
cp ./kube-proxy* /etc/kubernetes/ssl/
#*
~~~


- distribute them to /etc/kubernetes/ssl 

2. Generate kubeconfig file

[root@localhost kube-proxy]# cat generateKubeConfig.sh
#!/bin/bash

kubectl config set-cluster kubernetes \
--certificate-authority=/etc/etcd/ssl/ca.pem \
--embed-certs=true \
--server=https://192.168.94.134:6443 \
--kubeconfig=kube-proxy.kubeconfig
# 设置客户端认证参数
kubectl config set-credentials kube-proxy \
--client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
--client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
--embed-certs=true \
--kubeconfig=kube-proxy.kubeconfig
# 设置上下文参数
kubectl config set-context default \
--cluster=kubernetes \
--user=kube-proxy \
--kubeconfig=kube-proxy.kubeconfig
# 设置默认上下文
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
cp kube-proxy.kubeconfig /etc/kubernetes/

~~~

We should copy this file to all nodes.


3. Make WorkingDirectory : 


 mkdir -p /var/lib/kube-proxy

4. Edit kube-proxy.service

 [root@localhost kube-proxy]# cat /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/bin/kube-proxy \
  --bind-address=192.168.94.134 \
  --hostname-override=192.168.94.134 \
  --cluster-cidr=172.30.0.0/16 \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
  --logtostderr=true \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

5. Start Service


